# LLM-Sprint-Project

This repo is for displaying some works to be done related to Large Language Models (LLMs). Specifically:
- It is an extension of the previous porject (evaluate LLMs' learnability and evaluation ability).
  - LLMs in different langauges will be used for performing same tasks, and compare result.
  - LLMs may be trained or evaluated to see why there is a deviation in terms of token quality evaluation, compare to human evaluation.
- It may records some findings from the research papers.
  - The first paper in reviewing process: [JudgeLM](https://arxiv.org/pdf/2310.17631.pdf)
 

A copy of the previous project report is also available in repo.
